\documentclass[fleqn,reqno,10pt]{article}

\usepackage{CSPstyles}
\usepackage[margin=2cm]{geometry}
\usepackage[natbib=true,backend=biber]{biblatex}
\bibliography{project-bib.bib}

\newcommand{\pt}[1]{\textcolor{CSP-accent-1}{[PT: #1]}}
\newcommand{\faca}[1]{\textcolor{CSP-accent-2}{[FC: #1]}}


\title{Project ideas}
\author{Michael Franke, Polina Tsvilodub, Andreas Waldis}
\date{Last compiled: \today}

\begin{document}
\maketitle

\section{Methodological Robustness of Assessing Information within LMs}

This project is about implementing and verifying experimental code to study the robustness of methods to assess how information is encoded and propagated within language models. 

\paragraph{Background}
A language model transforms information from input tokens across its layers to produce a subsequent output text. 
Given this fundamental role of information, it is essential to study what types of information these models contain, which portions are relevant to producing the output, and whether this insight aligns with human intuition for the specific task these models are given. 
Typically, we use a simple linear model (\textit{probes}) as sensors to approximate information within language models \citep{belinkov-2022-probing}. 
For example, we use internal representations and train linear models on the toxicity level of the input text to determine whether information in the input internals correlates with the input toxicity, as in \citep{waldis2025aligned}. 
Since we need data representing the to-be-probed property (such as the input toxicity), the results and subsequent conclusions are entangled with the specific probing task design. 
In the case of toxicity, where we have a continuous value ranging from 0 (not toxic) to 1 (toxic), we intuitively formulate the probing task as a linear regression problem. 
Alternatively, we could bin the toxicity scores into three classes (\textit{low}, \textit{medium}, and \textit{high}) and use a logistic regression as a probe. 
But in doing so, we ignore that the ordering of these three classes is relevant. 
Accounting for the fact, ordinal regression considers the fact that the \textit{low} class is not only another class than \textit{medium} and \textit{high} but really \textbf{lower}.

\paragraph{Project Details}
Given these different ways of probing information, we may reach different conclusions (for example, which layer encodes the most information about toxicity) depending on the specific probing task design, such as linear, logistic, or ordinal regression. 
The goal of this project is to provide an experimental ground for comparing such varied probing designs and to verify the resulting code with a small case study. 
Specifically, it could include the following parts:
\begin{itemize}
    \item Familiarize with the existing code base (Python).
    \item Implementing dataset transformation from continuous scores to classes. 
    \item Implementing the ordinal regression model. 
    \item Implementing the code to support other language models, like encoder-decoder or text-diffusion models.
    \item Further abstract the core functionalities of the code to make it more reusable, including: 
    \begin{itemize}
        \item probing dataset structure
        \item model inference and dumping of internals
        \item running probing experiments
        \item automatically gathering and aggregating results
        \item  intervention on model internals to verify that information is causally relevant
    \end{itemize}
    \item Case study, where we apply all these steps to verify these steps using a simple setup of two probing tasks and two different language models.
\end{itemize}


\section{Crowd-Sourcing Model Internal Interpretability Research}
This project aims to build a codebase that allows saving and donating model internals (such as internal representations or string probabilities) during inference, as well as a small UI/API that allows other researchers to explore these donated internals based on metadata like specific language model, input prompt, generated text, text toxicity, and text sentiment. 

\paragraph{Background}
Current research predominantly relies on model behavior (generated text given a specific input) to evaluate language models. 
However, as inputs and outputs are often hard to exactly reproduce, we incur substantial computational expense, as model internals (emerging during the forward pass) are not stored since they are typically not in the research scope. 
As a result, interpretability research that focuses on a comprehensive model perspective and jointly assesses model behavior and internals cannot benefit from the majority of the conducted experiments.
Instead, one needs to replicate and rerun behavioral experiments to access the model internals and study models comprehensively. 

\paragraph{Project}

The central goal of this project is to build a small code library and a minimal demonstrator that can be easily integrated into behavioral assessments of language models. 
Specifically, this could include the following parts:
\begin{itemize}
    \item Allowing to automatically save model internals along with metadata, when using language models.
    \item Setting up a simple data structure to store these dumps
    \item Implementing a simple UI or API that allows querying specific internals of interest. For example, I would like to get all the internals from a particular task. 
\end{itemize}



\section{LMs for prior elicitation for probabilistic cognitive models}

\begin{enumerate}
  \item prior elicitation experiments are used to learn about what humans believe, because this is useful for many applications:
  \begin{itemize}
    \item in Bayesian statistical models, we sometimes want expert opinions to inform the priors over relevant paramerters
    \item probabilistic cognitive models often rely on human data to estimate (statistical) world knowledge, so that the model can make adequate predictions about more downstream variables of interest
    \item it may often also be an end in itself to see what people (individuals or the crowd on average) beliefs
  \end{itemize}
  \item there is a large strand of literature on experimental techniques for eliciting prior information (e.g., economics, psychometrics, cognitive science \dots)
  \item running prior elicitation experiments is costly, and there are many degrees of freedom in eliciting responses and in interpreting raw data (how to scale / interpret slider ratings etc.)
  \item this project could investigate if we can use LLMs, properly prompted, to elicit prior judgements instead of human participants
  \item this would be exploratory work (hence creative and fun) and could involve:
  \begin{itemize}
    \item use extant data sets from humans (available!) to compare against LMs
    \item try different prompting methods or other ways of operationalizing the task
    \item try in-context learning
    \item try out different personalities in system prompt
    \item check if judgements agree with human judgements
    \item check if model predictions are worse or better than with human priors
  \end{itemize}
\end{enumerate}

\section{Relevance judgements and LLMs}

\begin{enumerate}
  \item similar to the previous project, but focusing on human judgements of relevance
  \begin{itemize}
    \item relevance judgements are relevant for, e.g., reinforcement learning with machine feedback or open-ended cognitive modeling
  \end{itemize}
  \item probe the human-likeness of LLM's relevance judgements w/ and w/o prompting
  \item compare to human data (which we already have, e.g., \citet{WarstadtAgha2022:Testing-Bayesia})
\end{enumerate}

% \section{How false beliefs spread through language}

% \subsection{Argumentative RSA agents}

% \begin{itemize}
%   \item run a multi-agent simulation, similar to prior literature \citep{Zollman2013:Network-Epistem}, on how false beliefs spread and amplify in a social network
%   \begin{itemize}
%     \item agents observe data from experiments
%     \item in previous work: agents showed their data to other agents
%     \item here: agents describe their findings in language
%     \item use RSA model with argumentative speakers
%     \item listeners can be naive or argumentation-aware
%     \item check impact on false belief propagation
%   \end{itemize}
% \end{itemize}

% \subsection{Argumentative RSA agents}

% \begin{itemize}
%   \item as before but with griceChain agents instead of RSA
% \end{itemize}

\section{LLMs and causal reasoning}

\subsection{Squeeze causal intuitions out of LLMs}

\begin{itemize}
  \item use LLMs to ``construct'' intuitive causal models of everyday events
  \item test whether there are biases (e.g., towards specific causal structures)
\end{itemize}


\subsection{Compare causal language use in humans and LMs}

\begin{itemize}
  \item there is a growing experimental literature on \emph{causal attribution}, which is the problem of selecting one or few causes from (in principle) unboundedly many physical causes of any given event
  \item humans have shared preferences for which event to single out as ``the'' (single) actual case
  \item these intuitions seems to be informed by both statistical frequencies of events and normative factors \citep{IcardKominsky2017:Normality-and-a,QuillienLucas2023:Counterfactuals}
  \item this project would investigate whether current LMs show the same behavioral patterns as humans in identifying actual causes
  \begin{itemize}
    \item involves scavenging the literature for experimental material
    \item possibly generating new material (to avoid data contamination), maybe using LMs for automatization
    \item running a benchmark test (multiple-choice) with some current SOTA LMs
    \item benchmark data set could be publicly shared if curated well enough
  \end{itemize}
\end{itemize}


\section{Literature surveys}

\textbf{Remark:} You might think that it is a dull project to read a bunch of papers on the same topic, summarize and critically discuss them.
You may think that this is what losers do.
Or people from the far past, like in 2019.
BUT: if you do, you couldn't be more wrong.
With current intelligent assistants that can code, what the future needs are people to read the trends and decide on what is missing from or wrong with the status quo.
A literature project trains exactly this skill, more than any other more practical project would.

\begin{itemize}
  \item in a literature survey you start with 3-5 key papers on a single topic (ask us if you need a hint for finding a starting point!)
  \item you develop a deep understanding of the topic, possibly search for more papers on it
  \item you summarize different approaches or positions, and build your own critical opinion on benefits, problems, omissions etc.
  \item this is an exercise in deep work, critical thinking and clear writing (all of which are invaluable skills)
  \item topics you could look at:
  \begin{itemize}
    \item surprisal theory, next-word probabilities and modern LLMs (maybe branching out to calibration)
    \item current state of discussion on calibration of LLMs
    \item could or should LMs replace experimental participants in psychology or neighboring areas?
    \item overview over one or several recent techniques for mechanistic interpretability
    \item overview over cognitive agent models incorporating LMs
    \item overview over LM abilities in a particular domain (Theory of Mind, syntax, causal reasoning, ethical judgements \dots)
    \item best practices of LM evaluation with benchmarks or behavioral experiments
    \item recent discussion about in-context impersonation (how LMs might be used to mimic particular subpopulations (angry white men \dots))
  \end{itemize}
\end{itemize}

\section{Methods for assessing LM predictions in multiple-choice experiments}

\begin{itemize}
  \item this project would scale up existing work on the comparison of different methods of determining the predictions of an LM for a multiple-choice task \citep{TsvilodubWang2024:Predictions-fro}
  \item while the previous work only look at a few LMs and a very small selection of data sets, this project would extend the methodological comparison to a larger set of LMs and data sets (e.g., from BigBench)
  \item the project could also additionally pay mind to ``prediction calibration methods'' \citep{ZhaoWallace2021:Calibrate-Befor,HoltzmanWest2021:Surface-Form-Co}
\end{itemize}

\section{LLM agents}

An increasing body of work employs LLMs as part of larger computational systems, e.g., by equipping LLMs with ``tools'' (e.g., code compiler etc), or by building agents with modules like memory (e.g., generative agents discussed in class \citep{park2023generative}). 
Further, informed by cognitive science, LLMs are increasingly used as part of cognitively motivated architectures and models of human cognition. 
Building on a proof-of-concept example of referentia expression generation in a cognitively inspired process model \citep{tsvilodub2024cognitivemodelingscaffoldedllms}, below are some project ideas which would help expand and substantiate the framework.

\subsection{Testing i/o}

%Consists of testing griceChain on other datasets or evaluating the performance in general. Requires no or little change to the current griceChain codebase.

\begin{enumerate}
  \item Extend the experiment by \citet{tsvilodub2024cognitivemodelingscaffoldedllms} to another image captioning dataset (e.g., a subset of MS COCO).  %Applying the griceChain implementation of the contrastive reference game to more pre-existing datasets.
  \begin{itemize}
  	\item Use an image captioning module in order to sample possible message proposals.
  	\item Use an open-source LLM as the backbone for the modules. This will also allow to evaluate to which extent such frameworks work with open-source LLMs, since many published agents rely on GPT models.
  	\item Apply the framework to the chosen dataset.
  	\item Carefully evaluate the results against alternative models (e.g., off-the-shelf image captioning), using different metrics (e.g., standard language generation metrics, but also preferences for captions produced by either approach)
%    \item To extend the reference game or grounded language use setting to more naturalistic datasets, Google search images or MS COCO images could be used.
  %  \item Integrating the image captioning endpoint for the utterance proposal based on actual images can be part of the project or be implemented by us.
  \end{itemize}
  %\item Collecting/constructing a dataset for semantic parsing
 % \begin{itemize}
   % \item Currently, the semantic parsing tests contain both examples from SuperGLUE and GLUE benchmarks as well as hand-created examples representative of the modeled communicative tasks. However, for the former, sometimes the annotated truth values need to be changed compared to the original benchmark depending on the prompt / question phrasing. For the latter, more examples could be created. Students could create larger versions of such parsing datasets.
  %\end{itemize}
  %\item Applying the pipeline with other LLM backbones to check for possible advantages for models weaker than GPT-X
 % \begin{itemize}
  %  \item LLaMA, OpenAssistant, GPT4All...
  %  \item involves benchmarking their instruction-following / few-shot learning abilities to GPT
  %\end{itemize}
  %\item Accuracy and variance estimation of the pipeline on a given task or for given sub-components
  %\begin{itemize}
   % \item for a module like the semantic parser, there is no available estimate of the variance of results across different calls of the API, as well as across different sampling temperature sampling or tested phenomena. Having this information based on rigorous testing would be very useful for estimating the performance of the overall system.
   % \item this requires sizable datasets of different phenomena as well as resources for running the evaluations.
  %  \item one concern might be how generalizable / useful these results are across models, especially if we want to offer the pipeline as a cross-model solution and need to guarantee stable performance.
 % \end{itemize}
\end{enumerate}

\subsection{Building an argumentative agent}

%Require more substantial code extensions/revisions, but is based mostly on previous literature. E.g., implement some other RSA model with SIFD.

\begin{enumerate}
  \item previous work has shown that, given their training data, LLMs tend to reflect certain (e.g., political) opinions \citep{santurkar2023opinionslanguagemodelsreflect}. This project would investigate whether, taking inspiration from social reasoning in computational pragmatics \citep{yoon2020polite}, LLM agents as above could be used to adapt the generations to more diverse generations. %Extend griceChain to non-contrastive reference games.
  %\item Integrating new (e.g., more natural) communicative tasks beyond reference. Extending the pipeline to more complex or natural tasks could allow us to show the advantages of the controlled pipeline more clearly. This would require a bigger refactoring/extension of the current codebase.
  \begin{itemize}
    \item Based on the work above, come up with a task analysis for constructing generations that would, e.g., address certain topics or meet certain social goals. This can be achieved, e.g., by sampling different proposals from the LLM with different prompts and coming up with different evaluation and weighing schemes for the results.   %One idea that we discussed concerned the use of different utility functions.
    \item Based on a curated set of inputs and criteria for good generations (coming up with those would be aprt of the project), the project would build the framework, and carefully evaluate it against a baseline (e.g., vanilla inference with an LLM). 
   % \begin{itemize}
     % \item One natural extension here is to add argumentative strength to the utility function; the reason that this is natural is that argstrength in a Bayes factor/likratio implementation grounds out in a combo of semantic parser and state proposal. To implement this we need to add a state-proposer that proposes new states compatible with the utterance. For more look at the paper on argstrength RSA.
    %  \item Another idea is to use relevance (TODO)
    %\end{itemize}
   % \item Another idea is to generate possible speaker goals/preferences based on the utterance. For instance, answering wh-questions (or questions in general) might require reasoning about the speaker's background knowledge (e.g., sensible information or goals the person might want to pursue). \pt{This is essentially a transfer / extension of the QA model to SIFD} Furthermore, the iterative pipeline might allow to model QA based on conversation history which updates goals etc rather than single-shot QA.
   % \item One other idea could be to zoom in on the process of reasoning about vague expressions like gradable adjectives. For instance, given descriptions of utterance context, the pipeline might trade off between contextual information and general world knowledge for inferring aspects like the comparison class. E.g. a griceChain implementation of Qing \& Franke (2014).
  \end{itemize}
\end{enumerate}

\subsection{Evaluating self-improvement of agents}
\begin{enumerate}
	\item An increasing number of LLM agent architectures relies on self-improvement of LLMs, i.e., the quality of LLM-generated improvements, given LLMs' own feedback and previous samples, as, e.g., in the self-critique component of \citet{bai2022constitutionalaiharmlessnessai}. This project would take a closer look at the self-critiqueing quality.
	\begin{itemize}
		\item The goal of the project would be to construct a dataset consisting of systematic variations of the prompts, where different aspects of the prompt are `wrong' (e.g., the prompt is one-sided, incomplete, contains a wrong assumption, ... )
		\item Then, different LLMs or different prompting approaches are tested to see whether 1. the short-comings are identified correctly and 2. are corrected correcty, and which types of errors are more difficult to capture for the LMs.
	\end{itemize}
\end{enumerate}

%\subsection{Improving current griceChain implementation for discriminative reference games}

%This involves fairly substantial revisions to the code \& possibly even conceptual work.  Student would change griceChain in some way to improve the performance on current task. Possibly extend SIFD paradigm.

%\begin{enumerate}
 % \item Clarification questions: cf. self-critical and revising agents
 % \begin{itemize}
  %  \item The pragmatic module could be augmented by reasoning about which action next to picking one of the available utterances to select. These alternative actions could be, e.g., asking clarification questions. This could be guided be reasoning about the uncertainty of the current information. \pt{uncertainty in LLMs is quite a big topic by itself; modeling the reasoning is also a non-trivial task, which could be solved most naively by recursively applying the SIFD pipeline to compare utilities od possible outcomes of different actions}
 % \end{itemize}
  %\item ``Loopy agent'' going back to the step of generating possible utterance proposals whenever current set of options isn't useful
 % \begin{itemize}
  %  \item Involves the issue of putting a cap on the number of iterations the agent could loop for.
  %  \item Otherwise, a relatively straightforward recurrence over our current approach.
 % \end{itemize}
%\end{enumerate}

%\subsection{Real world application}

%\begin{enumerate}
%  \item Finding some real world use case for griceChain, exploiting its advantages over bare LLMs in terms of explainability and transparency.
%\end{enumerate}

%\section{Probing LLMs}
%\begin{itemize}
%  \item Probing LLMs seems like one of the straightforward methods for investigating the representations they might be building up.
%  \item Following up on the discussion of whether LLMs do develop some form of understanding or reasoning and how to find that out, it would be great to get into the common methods hands-on.
%  \item \pt{concrete applications tbd.}
%\end{itemize}

\section{Learning multi-hop reasoning from linguistic feedback}
\begin{enumerate}
	\item LMs often struggle with multi-hop question answering, like answering questions of the style ``Is the voice of the Genie from Disneyâ€™s Aladdin still alive?'', even with chain-of-thought prompting.
	\item One natural way to attempt to improve the performance is to provide llinguistic feedback correcting intermediate steps, given sample trajectories.
	\item The project would attempt to fine-tune a model (e.g., GPT-2) for multi-hop QA (from some available dataset), utilizing linguistic feedback in the style of inverse reinforcement learning. The feedback integration could be adapted from \citep{sumers2021learning}
	\item The feedback could be elicited from humans, or, for easier exploration, e.g., from GPT-4o.
	\item The bigger question that this project could ask is: given different kinds of feedback, does the model learn certain (abstract) strategies? (e.g., always, first gathering all facts, and then combining them? decomposing the problem in a human-like way?)
\end{enumerate}

%\section{System 1 vs. System 2 prompting / reasoning \& LLMs}
%\begin{enumerate}
%	\item Researchers in the field have stated that the difference between zero-shot and CoT prompting in LLMs is the same one as between system 1 and system 2 reasoning of humans.  It seems that this assumption has been accommodated into a lot of work on prompting strategies. The goal of this project is to conduct careful empirical or conceptual comparison between huamns and LMs.
%	\item If there is (accessible) human data of humans performing the same task that could be easily converted into text in different contexts (i.e., intuitive S1 condition,~vs.~ ``attentive'' S2 condition), it would be fun to compare whether human S1 / S2 results align with LLM zero-shot / CoT results.
%	\item \textcolor{gray}{(extensions, e.g., for theses)} If the first part is feasible, a natural extension would be to try to train a model (e.g., with hierarchical RL) which would learn to flexibly switch between zero-shot and CoT task solutions (i.e., learn to "prompt itself" with CoT, when necessary). This would probably require introducing some cost term for the CoT, so that it's not exploted. Then, the task would be to analyse whether the ``switch between systems'' is human-like.  This would at least address the question whether systems can learn to flexibly switch between different generation modes in an economic / efficient way.
%\end{enumerate}

\section{LLMs' sensitivity to social language cues}
\begin{enumerate}
	\item Inspired by work by \citet{burnett2019signalling} and \citet{beltrama2021imprecision}, this project investigates whether LLMs are sensitive to the (social) persona of the speaker, and whether the persona information affects how LLMs interpret speaker inputs.  
	\item The project would (a) replicate the behavioral experiment by \citet{beltrama2021imprecision} wherein the interpretation of imprecision expressions is tested, given nerdy vs. chill speaker personas. (b) Test this across different conditions (e.g., different persona descriptions, whether the model takes on similar behavior if it is prompted to take on the persona itself) (c) Utilize attribution methods to investigate what drives model predictions.
	\item The particular case study can be put in context of work like \citet{liu2024largelanguagemodelsnavigate}.
\end{enumerate}

\section{RLHF with model-based approaches}
\textcolor{gray}{NOTE: This is a larger thesis project.}
\begin{enumerate}
	\item This might be a more advanced, most abstract project. The idea is that more-human like learning is based on re-using learned representations and information across different tasks / situations, which leads to more generalizable representations. 
	\item The idea of the project would be to try to extend RLHF to apply (an approximation of) model-based RL, where the agent has to learn a model of the environment (intuitively, it should represent some more generalizable aspects of the environment). 
	\item The project would add some additional objective, e.g., an additional prediction task or a discrete bottleneck that would regularize towards more human-like representation learning. The project would test whether that leads to more human-like behavior of the LM (e.g., longer dialogues, or more diverse linguistic output, ...), as opposed to vanilla (more reward-hacking prone) fine-tuning.  One potential more concrete application could be personas (i.e., training models to take on personas consistently over longer interactions by making them maintain a listener model).
\end{enumerate}

\printbibliography[heading=bibintoc]

\end{document}
