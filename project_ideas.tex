\documentclass[fleqn,reqno,10pt]{article}

\usepackage{CSPstyles}
\usepackage[margin=2cm]{geometry}
\usepackage[natbib=true,backend=biber]{biblatex}
\bibliography{project-bib.bib}

\newcommand{\pt}[1]{\textcolor{CSP-accent-1}{[PT: #1]}}
\newcommand{\faca}[1]{\textcolor{CSP-accent-2}{[FC: #1]}}


\title{Project ideas}
\author{CSP}
\date{Last compiled: \today}

\begin{document}
\maketitle

\section{LMs for prior elicitation for probabilistic cognitive models}

\begin{enumerate}
  \item prior elicitation experiments are used to learn about what humans believe, because this is useful for many applications:
  \begin{itemize}
    \item in Bayesian statistical models, we sometimes want expert opinions to inform the priors over relevant paramerters
    \item probabilistic cognitive models often rely on human data to estimate (statistical) world knowledge, so that the model can make adequate predictions about more downstream variables of interest
    \item it may often also be an end in itself to see what people (individuals or the crowd on average) beliefs
  \end{itemize}
  \item there is a large strand of literature on experimental techniques for eliciting prior information (e.g., economics, psychometrics, cognitive science \dots)
  \item running prior elicitation experiments is costly, and there are many degrees of freedom in eliciting responses and in interpreting raw data (how to scale / interpret slider ratings etc.)
  \item this project could investigate if we can use LLMs, properly prompted, to elicit prior judgements instead of human participants
  \item this would be exploratory work (hence creative and fun) and could involve:
  \begin{itemize}
    \item use extant data sets from humans (available!) to compare against LMs
    \item try different prompting methods or other ways of operationalizing the task
    \item try in-context learning
    \item try out different personalities in system prompt
    \item check if judgements agree with human judgements
    \item check if model predictions are worse or better than with human priors
  \end{itemize}
\end{enumerate}

\section{Relevance judgements and LLMs}

\begin{enumerate}
  \item similar to the previous project, but focusing on human judgements of relevance
  \begin{itemize}
    \item relevance judgements are relevant for, e.g., reinforcement learning with machine feedback or open-ended cognitive modeling
  \end{itemize}
  \item probe the human-likeness of LLM's relevance judgements w/ and w/o prompting
  \item compare to human data (which we already have)
\end{enumerate}

% \section{How false beliefs spread through language}

% \subsection{Argumentative RSA agents}

% \begin{itemize}
%   \item run a multi-agent simulation, similar to prior literature \citep{Zollman2013:Network-Epistem}, on how false beliefs spread and amplify in a social network
%   \begin{itemize}
%     \item agents observe data from experiments
%     \item in previous work: agents showed their data to other agents
%     \item here: agents describe their findings in language
%     \item use RSA model with argumentative speakers
%     \item listeners can be naive or argumentation-aware
%     \item check impact on false belief propagation
%   \end{itemize}
% \end{itemize}

% \subsection{Argumentative RSA agents}

% \begin{itemize}
%   \item as before but with griceChain agents instead of RSA
% \end{itemize}

\section{LLMs and causal reasoning}

\subsection{Squeeze causal intuitions out of LLMs}

\begin{itemize}
  \item use LLMs to ``construct'' intuitive causal models of everyday events
  \item test whether there are biases (e.g., towards specific causal structures)
\end{itemize}


\subsection{Compare causal language use in humans and LMs}

\begin{itemize}
  \item there is a growing experimental literature on \emph{causal attribution}, which is the problem of selecting one or few causes from (in principle) unboundedly many physical causes of any given event
  \item humans have shared preferences for which event to single out as ``the'' (single) actual case
  \item these intuitions seems to be informed by both statistical frequencies of events and normative factors \citep{IcardKominsky2017:Normality-and-a,QuillienLucas2023:Counterfactuals}
  \item this project would investigate whether current LMs show the same behavioral patterns as humans in identifying actual causes
  \begin{itemize}
    \item involves scavenging the literature for experimental material
    \item possibly generating new material (to avoid data contamination), maybe using LMs for automatization
    \item running a benchmark test (multiple-choice) with some current SOTA LMs
    \item benchmark data set could be publicly shared if curated well enough
  \end{itemize}
\end{itemize}


\section{Literature surveys}

\textbf{Remark:} You might think that it is a dull project to read a bunch of papers on the same topic, summarize and critically discuss them.
You may think that this is what losers do.
Or people from the far past, like in 2019.
BUT: if you do, you couldn't be more wrong.
With current intelligent assistants that can code, what the future needs are people to read the trends and decide on what is missing from or wrong with the status quo.
A literature project trains exactly this skill, more than any other more practical project would.

\begin{itemize}
  \item in a literature survey you start with 3-5 key papers on a single topic
  \item you develop a deep understanding of the topic, possibly search for more papers on it
  \item you summarize different approaches or positions, and build your own critical opinion on benefits, problems, omissions etc.
  \item this is an exercise in deep work, critical thinking and clear writing (all of which are invaluable skills)
  \item topics you could look at:
  \begin{itemize}
    \item surprisal theory, next-word probabilities and modern LLMs (maybe branching out to calibration)
    \item current state of discussion on calibration of LLMs
    \item could or should LMs replace experimental participants in psychology or neighboring areas?
    \item overview over one or several recent techniques for mechanistic interpretability
    \item overview over cognitive agent models incorporating LMs
    \item overview over LM abilities in a particular domain (Theory of Mind, syntax, causal reasoning, ethical judgements \dots)
    \item best practices of LM evaluation with benchmarks or behavioral experiments
    \item recent discussion about in-context impersonation (how LMs might be used to mimic particular subpopulations (angry white men \dots))
  \end{itemize}
\end{itemize}

\section{Methods for assessing LM predictions in multiple-choice experiments}

\begin{itemize}
  \item this project would scale up existing work on the comparison of different methods of determining the predictions of an LM for a multiple-choice task \citep{TsvilodubWang2024:Predictions-fro}
  \item while the previous work only look at a few LMs and a very small selection of data sets, this project would extend the methodological comparison to a larger set of LMs and data sets (e.g., from BigBench)
  \item the project could also additionally pay mind to ``prediction calibration methods'' \citep{ZhaoWallace2021:Calibrate-Befor,HoltzmanWest2021:Surface-Form-Co}
\end{itemize}

\section{LLM-chain related}

Below are some student project ideas which would help expand and substantiate the framework.

\subsection{Testing i/o}

Consists of testing griceChain on other datasets or evaluating the performance in general. Requires no or little change to the current griceChain codebase.

\begin{enumerate}
  \item Applying the griceChain implementation of the contrastive reference game to more pre-existing datasets.
  \begin{itemize}
    \item To extend the reference game or grounded language use setting to more naturalistic datasets, Google search images or MS COCO images could be used.
    \item Integrating the image captioning endpoint for the utterance proposal based on actual images can be part of the project or be implemented by us.
  \end{itemize}
  \item Collecting/constructing a dataset for semantic parsing
  \begin{itemize}
    \item Currently, the semantic parsing tests contain both examples from SuperGLUE and GLUE benchmarks as well as hand-created examples representative of the modeled communicative tasks. However, for the former, sometimes the annotated truth values need to be changed compared to the original benchmark depending on the prompt / question phrasing. For the latter, more examples could be created. Students could create larger versions of such parsing datasets.
  \end{itemize}
  \item Applying the pipeline with other LLM backbones to check for possible advantages for models weaker than GPT-X
  \begin{itemize}
    \item LLaMA, OpenAssistant, GPT4All...
    \item involves benchmarking their instruction-following / few-shot learning abilities to GPT
  \end{itemize}
  \item Accuracy and variance estimation of the pipeline on a given task or for given sub-components
  \begin{itemize}
    \item for a module like the semantic parser, there is no available estimate of the variance of results across different calls of the API, as well as across different sampling temperature sampling or tested phenomena. Having this information based on rigorous testing would be very useful for estimating the performance of the overall system.
    \item this requires sizable datasets of different phenomena as well as resources for running the evaluations.
    \item one concern might be how generalizable / useful these results are across models, especially if we want to offer the pipeline as a cross-model solution and need to guarantee stable performance.
  \end{itemize}
\end{enumerate}

\subsection{Extending the model to more tasks}

Require more substantial code extensions/revisions, but is based mostly on previous literature. E.g., implement some other RSA model with SIFD.

\begin{enumerate}
  \item Extend griceChain to non-contrastive reference games.
  \item Integrating new (e.g., more natural) communicative tasks beyond reference. Extending the pipeline to more complex or natural tasks could allow us to show the advantages of the controlled pipeline more clearly. This would require a bigger refactoring/extension of the current codebase.
  \begin{itemize}
    \item One idea that we discussed concerned the use of different utility functions.
    \begin{itemize}
      \item One natural extension here is to add argumentative strength to the utility function; the reason that this is natural is that argstrength in a Bayes factor/likratio implementation grounds out in a combo of semantic parser and state proposal. To implement this we need to add a state-proposer that proposes new states compatible with the utterance. For more look at the paper on argstrength RSA.
      \item Another idea is to use relevance (TODO)
    \end{itemize}
    \item Another idea is to generate possible speaker goals/preferences based on the utterance. For instance, answering wh-questions (or questions in general) might require reasoning about the speaker's background knowledge (e.g., sensible information or goals the person might want to pursue). \pt{This is essentially a transfer / extension of the QA model to SIFD} Furthermore, the iterative pipeline might allow to model QA based on conversation history which updates goals etc rather than single-shot QA.
    \item One other idea could be to zoom in on the process of reasoning about vague expressions like gradable adjectives. For instance, given descriptions of utterance context, the pipeline might trade off between contextual information and general world knowledge for inferring aspects like the comparison class. E.g. a griceChain implementation of Qing \& Franke (2014).
  \end{itemize}
\end{enumerate}

\subsection{Improving current griceChain implementation for discriminative reference games}

This involves fairly substantial revisions to the code \& possibly even conceptual work.  Student would change griceChain in some way to improve the performance on current task. Possibly extend SIFD paradigm.

\begin{enumerate}
  \item Clarification questions: cf. self-critical and revising agents
  \begin{itemize}
    \item The pragmatic module could be augmented by reasoning about which action next to picking one of the available utterances to select. These alternative actions could be, e.g., asking clarification questions. This could be guided be reasoning about the uncertainty of the current information. \pt{uncertainty in LLMs is quite a big topic by itself; modeling the reasoning is also a non-trivial task, which could be solved most naively by recursively applying the SIFD pipeline to compare utilities od possible outcomes of different actions}
  \end{itemize}
  \item ``Loopy agent'' going back to the step of generating possible utterance proposals whenever current set of options isn't useful
  \begin{itemize}
    \item Involves the issue of putting a cap on the number of iterations the agent could loop for.
    \item Otherwise, a relatively straightforward recurrence over our current approach.
  \end{itemize}
\end{enumerate}

\subsection{Real world application}

\begin{enumerate}
  \item Finding some real world use case for griceChain, exploiting its advantages over bare LLMs in terms of explainability and transparency.
\end{enumerate}

\section{Probing LLMs}
\begin{itemize}
  \item Probing LLMs seems like one of the straightforward methods for investigating the representations they might be building up.
  \item Following up on the discussion of whether LLMs do develop some form of understanding or reasoning and how to find that out, it would be great to get into the common methods hands-on.
  \item \pt{concrete applications tbd.}
\end{itemize}

\section{Learning multi-hop reasoning from linguistic feedback}
\begin{enumerate}
	\item LMs often struggle with multi-hop question answering, like answering questions of the style ``Is the voice of the Genie from Disney’s Aladdin still alive?'', even with chain-of-thought prompting
	\item One natural way to attempt to improve the performance is to provide llinguistic feedback correcting intermediate steps, given sample trajectories.
	\item The project would attempt to fine-tune a model (e.g., GPT-2) for multi-hop QA (from some available dataset), utilizing linguistic feedback in the style of inverse reinforcement learning. The feedback integration could be adapted from \citep{sumers2021learning}
	\item The feedback could be elicited from humans, or, for easier exploration, e.g., from GPT-4o.
	\item The bigger question that this project could ask is: given different kinds of feedback, does the model learn certain (abstract) strategies? (e.g., always, first gathering all facts, and then combining them? decomposing the problem in a human-like way?)
\end{enumerate}

\section{System 1 vs. System 2 prompting / reasoning of LLMs}
\begin{enumerate}
	\item Noah once (half-)jokingly tweeted that CoT is the System-2 reasoning of LLMs. I haven't seen actual empirical investigations of that! It seems however that it has been accommodated into a lot of work on prompting strategies.
	\item If there is (accessible) human data of humans performing the same task that could be easily converted into text in different contexts (i.e., intuitive S1 condition,~vs.~ ``attentive'' S2 condition), it would be fun to compare whether human S1 / S2 results align with LLM zero-shot / CoT results.
	\item If the first part is feasible, a natural extension would be to try to train a model (e.g., with hierarchical RL) which would learn to flexibly switch between zero-shot and CoT task solutions (i.e., learn to "prompt itself" with CoT, when necessary). This would probably require introducing some cost term for the CoT, so that it's not exploted. Then, the task would be to analyse whether the ``switch between systems'' is human-like.  This would at least address the question whether systems can learn to flexibly switch between different generation modes in an economic / efficient way.
\end{enumerate}

\section{LLMs' sensitivity to social language cues}
\begin{enumerate}
	\item Inspired by work by \citet{burnett2019signalling} and \citet{beltrama2021imprecision}, this project investigates whether LLMs are sensitive to the (social) persona of the speaker, and whether the persona information affects how LLMs interpret speaker inputs.  
	\item The project would (a) replicate the behavioral experiment by \citet{beltrama2021imprecision} wherein the interpretation of imprecision expressions is tested, given nerdy vs. chill speaker personas. (b) Test this across different conditions (e.g., different persona descriptions, whether the model takes on similar behavior if it is prompted to take on the persona itself) (c) Utilize attribution methods to investigate what drives model predictions.
	\item The particular case study can be put in context of work like \citet{liu2024largelanguagemodelsnavigate}.
\end{enumerate}

\section{RLHF with model-based approaches}
\begin{enumerate}
	\item This might be a more advanced, most abstract project. The idea is that more-human like learning is based on re-using learned representations and information across different tasks / situations, which leads to more generalizable representations. 
	\item The idea of the project would be to try to extend RLHF to apply (an approximation of) model-based RL, where the agent has to learn a model of the environment (intuitively, it should represent some more generalizable aspects of the environment). 
	\item The project would add some additional objective, e.g., an additional prediction task or some bottleneck that would regularize towards more human-like representation learning. The project would test whether that leads to more human-like behavior of the LM (specific hypothesis TBD), as opposed to vanilla (more reward-hacking prone) fine-tuning.  One potential more concrete application could be personas (i.e., training models to take on personas consistently over longer interactions by making them maintain a listener model).
\end{enumerate}

\printbibliography[heading=bibintoc]

\end{document}
